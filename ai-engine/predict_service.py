import json
import os
import logging
import datetime
import sys
import pandas as pd
import numpy as np
from kafka import KafkaConsumer, KafkaProducer
from sklearn.linear_model import LinearRegression

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# Windowså¯¾ç­–
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

# è¨­å®šã®å¤–éƒ¨åŒ–
KAFKA_BROKER = os.getenv("KAFKA_BROKER", "localhost:9092")
KAFKA_TOPIC_INPUT = os.getenv("KAFKA_TOPIC_INPUT", "scouter.score.input")
KAFKA_TOPIC_OUTPUT = os.getenv("KAFKA_TOPIC_OUTPUT", "scouter.prediction.result")
GROUP_ID = os.getenv("KAFKA_GROUP_ID", "ai-engine-group")

class PredictionEngine:
    """ä½“èª¿äºˆæ¸¬ã‚’è¡Œã†AIã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def calculate_scouter_score(self, df: pd.DataFrame):
        """
        Javaå´ã®ã€å³æ ¼æŸ»å®šãƒ­ã‚¸ãƒƒã‚¯ã€ã‚’Pythonã§å†ç¾ã€‚
        """
        # 1. ç–²åŠ´ä»¥å¤–ã®6é …ç›®ã®å¹³å‡
        pos_cols = ['focus', 'efficiency', 'motivation', 'condition', 'sleepQuality', 'sexualDesire']
        base_avg = df[pos_cols].mean(axis=1)
        
        # 2. ç–²åŠ´ãƒšãƒŠãƒ«ãƒ†ã‚£ã®åŸºæœ¬å€¤ (0.0 ã€œ 1.0)
        fatigue_penalty = (df['fatigue'] - 1) / 6.0
        
        # --- ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒªãƒ“ãƒ‰ãƒ¼ãƒ¢ãƒ¼ãƒ‰(7): ã‚ªãƒ¼ãƒãƒ¼ãƒ’ãƒ¼ãƒˆä»•æ§˜ ---
        libido_mask = df['sexualDesire'] == 7
        base_avg.loc[libido_mask] = base_avg.loc[libido_mask] + 0.5 - 1.5
        fatigue_penalty.loc[libido_mask] = fatigue_penalty.loc[libido_mask] * 3.0
        
        # --- è³¢è€…ãƒ¢ãƒ¼ãƒ‰(1): å®‰å®šä»•æ§˜ ---
        sage_mask = df['sexualDesire'] == 1
        fatigue_penalty.loc[sage_mask] = fatigue_penalty.loc[sage_mask] * 0.5
        
        # æœ€çµ‚ã‚¹ã‚³ã‚¢ç®—å‡º
        final_score = base_avg - fatigue_penalty
        return final_score.clip(1.0, 7.0)

    def predict_weekly_condition(self, df_input: pd.DataFrame):
        """éå»ã®ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€å°†æ¥7æ—¥é–“ã®æ¨ç§»ã‚’äºˆæ¸¬ã™ã‚‹"""
        if len(df_input) < 2:
            logger.warning("Insufficient data for prediction (need at least 2 points).")
            return []

        try:
            # Javaå´ã®æ–°ãƒ­ã‚¸ãƒƒã‚¯ã«åŸºã¥ã„ãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’ç®—å‡º
            # ã“ã‚Œã«ã‚ˆã‚Šã€AIã¯ã€Œç„¡ç†ã‚’ã—ã¦ã„ã‚‹é«˜ã‚¹ã‚³ã‚¢ã€ã®å¾Œã®å¤±é€Ÿã‚’å­¦ç¿’å¯èƒ½ã«ãªã‚‹
            df_input['target_score'] = self.calculate_scouter_score(df_input)

            # æ—¥ä»˜å‡¦ç†
            df_input['date'] = pd.to_datetime(df_input['targetDate'])
            base_date = df_input['date'].min()
            df_input['days_passed'] = (df_input['date'] - base_date).dt.days

            # å­¦ç¿’ (ç·šå½¢å›å¸°)
            # ç›®çš„å¤‰æ•°ã‚’ 'condition' ã‹ã‚‰ 'target_score' ã«å¤‰æ›´
            X = df_input[['days_passed']]
            y = df_input['target_score']
            
            model = LinearRegression()
            model.fit(X, y)

            # äºˆæ¸¬ (å°†æ¥7æ—¥é–“)
            last_date = df_input['date'].max()
            future_dates = [last_date + datetime.timedelta(days=i) for i in range(1, 8)]
            future_days_passed = pd.DataFrame({
                'days_passed': [(d - base_date).days for d in future_dates]
            })

            predictions = model.predict(future_days_passed)
            
            # çµæœã®æ•´å½¢
            return [
                {
                    "date": d.strftime('%Y-%m-%d'), 
                    "predicted_score": float(np.clip(round(s, 2), 1.0, 7.0)) 
                }
                for d, s in zip(future_dates, predictions)
            ]
        except Exception as e:
            logger.error(f"Prediction logic error: {e}")
            return []

def run_service():
    """Kafkaã‚µãƒ¼ãƒ“ã‚¹å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
    engine = PredictionEngine()
    
    try:
        logger.info(f"ğŸš€ AI Engine Starting... Broker: {KAFKA_BROKER}")
        
        consumer = KafkaConsumer(
            KAFKA_TOPIC_INPUT,
            bootstrap_servers=KAFKA_BROKER,
            value_deserializer=lambda x: json.loads(x.decode('utf-8')),
            auto_offset_reset='latest',
            group_id=GROUP_ID
        )
        
        producer = KafkaProducer(
            bootstrap_servers=KAFKA_BROKER,
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )

        logger.info("âœ… Connected to Kafka! Waiting for messages...")

        for message in consumer:
            try:
                payload = message.value
                message_id = payload.get("messageId", "unknown")
                logger.info(f"ğŸ“© Received request. ID: {message_id}")
                
                history_data = payload.get('history', []) 
                if not history_data:
                    logger.warning(f"Empty history in message {message_id}")
                    continue

                df = pd.DataFrame(history_data)
                
                # äºˆæ¸¬å®Ÿè¡Œ
                prediction_results = engine.predict_weekly_condition(df)

                if prediction_results:
                    response = {
                        "messageId": message_id,
                        "predictions": prediction_results 
                    }
                    producer.send(KAFKA_TOPIC_OUTPUT, value=response)
                    producer.flush()
                    logger.info(f"ğŸ“¤ Sent prediction for ID: {message_id} ({len(prediction_results)} days)")

            except Exception as e:
                logger.error(f"Error processing message: {e}")

    except Exception as e:
        logger.critical(f"Failed to connect or run Kafka service: {e}")
        sys.exit(1)

if __name__ == "__main__":
    run_service()